{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078cfcaf16b44f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5ab9e618e770eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:53:43.669611Z",
     "start_time": "2025-04-25T08:53:43.550030Z"
    }
   },
   "source": [
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "f526e4195724ed8e",
   "metadata": {},
   "source": [
    "## Preprocessing & Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80e0694a45b504",
   "metadata": {},
   "source": [
    "Split the 'valid_time' into date and time"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e19859d825c7671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:53:43.793073Z",
     "start_time": "2025-04-25T08:53:43.789591Z"
    }
   },
   "source": [
    "def extract_datetime_features(df):\n",
    "    df['valid_time'] = pd.to_datetime(df['valid_time']) # Transfer 'valid_time' into datetime type\n",
    "    df['year'] = df['valid_time'].dt.year\n",
    "    df['month'] = df['valid_time'].dt.month\n",
    "    df['day'] = df['valid_time'].dt.day\n",
    "    df['hour'] = df['valid_time'].dt.hour\n",
    "    df[\"season\"] = ((df[\"month\"] % 12 + 3) // 3) + 1\n",
    "    return df.drop([\"id\"], axis=1)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "da9c00919be1ceec",
   "metadata": {},
   "source": "Add nearby features to strength the dataset, allowing the model to capture the continuity and spatial connection of the weather."
  },
  {
   "cell_type": "code",
   "id": "976d53493f917e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:53:43.923081Z",
     "start_time": "2025-04-25T08:53:43.918905Z"
    }
   },
   "source": [
    "# Add historical data for the specific place, time lag is one hour. Places without last hour's data results in N/A so will need to drop N/A after applying this.\n",
    "def add_lag_features(df, features, group_cols=['latitude', 'longitude'], lag=1):\n",
    "    df = df.sort_values(by=group_cols + ['valid_time']).reset_index(drop=True)\n",
    "    for feat in features:\n",
    "        df[f'{feat}_lag{lag}'] = df.groupby(group_cols)[feat].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Construct a 3*3 window for each spot to compute the average data for the nine neighbors.\n",
    "def add_spatial_averages(df, features, lat_step=0.25, lon_step=0.25):\n",
    "    avg_features = []\n",
    "    for feat in features:\n",
    "        df_list = []\n",
    "        for dlat in [-lat_step, 0, lat_step]:\n",
    "            for dlon in [-lon_step, 0, lon_step]:\n",
    "                shifted = df[['latitude', 'longitude', 'valid_time', feat]].copy()\n",
    "                shifted['latitude'] += dlat\n",
    "                shifted['longitude'] += dlon\n",
    "                shifted = shifted.rename(columns={feat: f'{feat}_shift_{dlat}_{dlon}'})\n",
    "                df_list.append(shifted)\n",
    "        for df_part in df_list:\n",
    "            df = df.merge(df_part, on=['latitude', 'longitude', 'valid_time'], how='left')\n",
    "        mean_cols = [f'{feat}_shift_{dlat}_{dlon}' for dlat in [-lat_step, 0, lat_step] for dlon in [-lon_step, 0, lon_step]]\n",
    "        df[f'{feat}_spatial_mean'] = df[mean_cols].mean(axis=1)\n",
    "        df = df.drop(columns=mean_cols)\n",
    "        avg_features.append(f'{feat}_spatial_mean')\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "47ebc7988633db2e",
   "metadata": {},
   "source": [
    "Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad77c5c6500fd649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:53:44.005562Z",
     "start_time": "2025-04-25T08:53:44.000903Z"
    }
   },
   "source": [
    "standard_features = [\n",
    "    'tp', 'sp', 'u10', 'v10', 'u100', 'v100', 'tcc', 'ptype',\n",
    "    'month', 'day', 'hour', 'season'\n",
    "]\n",
    "\n",
    "lag_vars = ['tp', 'sp', 'u10', 'v10']\n",
    "spatial_vars = ['tp', 'sp', 'tcc']\n",
    "\n",
    "# For consistency, define lag and spatial lag features\n",
    "lag_features = [f'{var}_lag1' for var in lag_vars]\n",
    "spatial_lag_features = [f'{var}_lag1_spatial_mean' for var in lag_vars]\n",
    "\n",
    "final_features = standard_features + lag_features + spatial_vars + spatial_lag_features\n",
    "\n",
    "# Load dataset and, split valid_time, sampling for model training.\n",
    "def prepare_dataset(option='standard', path='data/train.csv'):\n",
    "\n",
    "    df = pd.read_csv(path).copy()\n",
    "    df = extract_datetime_features(df)\n",
    "    df = df.sort_values(by=['latitude', 'longitude', 'valid_time']).reset_index(drop=True)\n",
    "\n",
    "    if option == 'standard':\n",
    "        df = df[standard_features + ['t2m']].dropna()\n",
    "        return df\n",
    "    if option == 'with_lag':\n",
    "        df = add_lag_features(df, lag_vars)\n",
    "        df = df[standard_features + lag_features + ['t2m']].dropna()\n",
    "        return df\n",
    "    if option == 'with_lag_spatial':\n",
    "        df = add_lag_features(df, lag_vars)\n",
    "        df = add_spatial_averages(df, spatial_vars)\n",
    "        df = add_spatial_averages(df, lag_features)\n",
    "        df = df[final_features + ['t2m']].dropna()\n",
    "        return df\n",
    "    raise ValueError(\"Invalid option. Choose from 'standard', 'with_lag', 'with_lag_spatial'\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "edaebcfbc3b2d909",
   "metadata": {},
   "source": "## Split sampled train dataset"
  },
  {
   "cell_type": "code",
   "id": "855f3e9b6c899a90",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-25T08:53:44.185262Z"
    }
   },
   "source": [
    "# 1. Load strengthened data and sampling.\n",
    "df = prepare_dataset('with_lag_spatial')\n",
    "\n",
    "# Check if the processed dataset already exists.\n",
    "if os.path.exists(\"df.csv\"):\n",
    "    df = pd.read_csv(\"df.csv\")\n",
    "    print(\"Loaded: df.csv\")\n",
    "else:\n",
    "    df.to_csv(\"df.csv\", index=False)\n",
    "    print(\"Saved: df.csv\")\n",
    "\n",
    "# Sampling for the fine-tuning\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# 2. Split train and validation set on the sample\n",
    "X_sample = df_sample.drop(columns=['t2m'])\n",
    "y_sample = df_sample['t2m']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5319b3642021cacc",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf012150cf63f4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:15:53.458805Z",
     "start_time": "2025-04-25T08:14:50.074096Z"
    }
   },
   "source": [
    "# Fallback Strategy, in case there's already a model\n",
    "if os.path.exists(\"rf_best_model.pkl\"):\n",
    "    rf_best_model = joblib.load(\"rf_best_model.pkl\")\n",
    "    print(\"Loaded existing tuned model: rf_best_model.pkl\")\n",
    "else:\n",
    "    start = time.perf_counter()\n",
    "    # Parameters\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_depth': [20, 30, 40],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Settings\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=1,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Tuning time: {end - start:.2f} seconds\")\n",
    "\n",
    "    # Find the best model\n",
    "    best_model = search.best_estimator_\n",
    "    joblib.dump(best_model, \"rf_best_model.pkl\")\n",
    "    print(\"Best sample model saved to rf_best_model.pkl\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Tuning time: 57.28 seconds\n",
      "Best sample model saved to rf_best_model.pkl\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation on the validation set",
   "id": "80b854787bfdc16d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:15:54.149192Z",
     "start_time": "2025-04-25T08:15:53.511545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation by three index\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"Validation RMSE: {rmse:.4f}, R¬≤: {r2:.4f}, MAE: {mae:.4f}\")\n",
    "print(\"Best Parameters:\", search.best_params_)"
   ],
   "id": "104acbdeed266199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.1980, R¬≤: 0.9405, MAE: 0.8358\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a30855dab7ea1aaf",
   "metadata": {},
   "source": "## Final Model trained on the whole dataset"
  },
  {
   "cell_type": "code",
   "id": "d0351405b6a78262",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-25T08:25:51.578464Z"
    }
   },
   "source": [
    "# 1. Features and target value\n",
    "X = df.drop(columns=['t2m'])\n",
    "y = df['t2m']\n",
    "X_full, X_val_full, y_full, y_val_full = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_full.to_csv(\"full_train_data.csv\", index=False)\n",
    "\n",
    "# Save copy of split train and validation sets\n",
    "X_full.to_csv(\"X_full.csv\", index=False)\n",
    "X_val_full.to_csv(\"X_val_full.csv\", index=False)\n",
    "y_full.to_csv(\"y_full.csv\", index=False)\n",
    "y_val_full.to_csv(\"y_val_full.csv\", index=False)\n",
    "print(\"Saved: training and validation splits on full dataset\")\n",
    "\n",
    "# Fallback\n",
    "if os.path.exists(\"rf_final_model.pkl\"):\n",
    "    final_model = joblib.load(\"rf_final_model.pkl\")\n",
    "    print(\"Loaded final model from rf_final_model.pkl\")\n",
    "else:\n",
    "    # 2. Construct final modelÔºàusing search.best_paramsÔºâ\n",
    "    start = time.perf_counter()\n",
    "    params = best_model.get_params()\n",
    "    params[\"n_jobs\"] = -1\n",
    "    params[\"random_state\"] = 42\n",
    "\n",
    "    # 3. Construct new model on the parameters\n",
    "    final_model = RandomForestRegressor(**params)\n",
    "\n",
    "    final_model.fit(X_full, y_full)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Final model training time: {end - start:.2f} seconds\")\n",
    "    joblib.dump(final_model, 'rf_final_model.pkl')\n",
    "    print(\"Final model trained on full dataset and saved as 'rf_final_model.pkl'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The model is evaluated by three scores: RMSE, R2, and MAE."
   ],
   "id": "b2993956ca6d56dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation metrics\n",
    "y_pred = final_model.predict(X_full)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_full, y_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_full, y_val_pred))\n",
    "train_r2 = r2_score(y_full, y_pred)\n",
    "val_r2 = r2_score(y_val_full, y_val_pred)\n",
    "train_mae = mean_absolute_error(y_full, y_pred)\n",
    "val_mae = mean_absolute_error(y_val_full, y_val_pred)\n",
    "\n",
    "print(\"Evaluations on Full Dataset\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Training R¬≤: {train_r2:.4f}\")\n",
    "print(f\"Validation R¬≤: {val_r2:.4f}\")\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")"
   ],
   "id": "7f82ae78083e8821"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualisation of evaluations\n",
    "# Create evaluation DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'RMSE': [train_rmse, val_rmse],\n",
    "    'R¬≤': [train_r2, val_r2],\n",
    "    'MAE': [train_mae, val_mae]\n",
    "}, index=['Train', 'Validation'])\n",
    "\n",
    "# Visualisation of all the evaluations\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, metric in enumerate(metrics_df.columns):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df[metric], palette='Set2')\n",
    "    plt.title(metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.suptitle(\"Model Evaluation Metrics on Train vs. Validation Sets\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ],
   "id": "bad20b3c38635f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictions on the full dataset",
   "id": "ad22264ac1da74a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Saving the validation predictions\n",
    "This will be later used in the final Evaluation file"
   ],
   "id": "4dafec61f2e9f2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predict on the full validation dataset\n",
    "print(\"Start Predicting‚Ä¶‚Ä¶\")\n",
    "y_val_pred = final_model.predict(X_val_full)\n",
    "print(\"Predicting doneÔºÅ\")\n",
    "\n",
    "# Save predictions and true values on full validation set\n",
    "df_val_results = pd.DataFrame({\n",
    "    'true_t2m': y_val_full.values,\n",
    "    'predicted_t2m': y_val_pred\n",
    "})\n",
    "\n",
    "df_val_results.to_csv('rf_y_valid_predictions.csv', index=False)\n",
    "print(\"Full validation results saved to 'rf_y_valid_predictions.csv'\")"
   ],
   "id": "8e68c98efcf69018"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save required final predictions",
   "id": "b98e8838b31b215c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print out the time this cell uses\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Read and process test set\n",
    "test_df = pd.read_csv('data/test.csv').copy()\n",
    "test_df = extract_datetime_features(test_df)\n",
    "test_df = test_df.sort_values(by=['latitude', 'longitude', 'valid_time']).reset_index(drop=True)\n",
    "\n",
    "# Add nearby features\n",
    "test_df = add_lag_features(test_df, lag_vars)\n",
    "test_df = add_spatial_averages(test_df, spatial_vars)\n",
    "test_df = add_spatial_averages(test_df, lag_features)\n",
    "test_df = test_df[final_features].dropna()\n",
    "# Save to csv\n",
    "test_df.to_csv('processed_test_with_nearby_features.csv', index=False)\n",
    "\n",
    "\n",
    "# Load the model and predict\n",
    "X_test = test_df[final_features]\n",
    "model = joblib.load(\"rf_final_model.pkl\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save to predictions.csv\n",
    "test_df['prediction'] = y_pred\n",
    "subset = test_df[(test_df['latitude'] == 56.25) & (test_df['longitude'] == -2.75)]\n",
    "submission = subset[['valid_time','prediction']]\n",
    "submission.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions saved as 'predictions.csv'\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Elapsed Time: {end - start:.2f} seconds\")"
   ],
   "id": "dee4ca5b84c4784b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Result Visualisation",
   "id": "7c0757531442e075"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Actual vs Predicted",
   "id": "cb1ea5b7ae0f63d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Plot training and validation in different colors\n",
    "sns.scatterplot(x=y_full, y=y_pred, label='Training data', alpha=0.3, color='blue')\n",
    "sns.scatterplot(x=y_val_full, y=y_val_pred, label='Validation data', alpha=0.3, color='red')\n",
    "\n",
    "# Perfect prediction line\n",
    "plt.plot([y_val_full.min(), y_val_full.max()], [y_val_full.min(), y_val_full.max()], 'k--', label='Perfect prediction')\n",
    "\n",
    "plt.xlabel('True t2m')\n",
    "plt.ylabel('Predicted t2m')\n",
    "plt.title('Prediction vs True Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a0569bd1d94968d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Residual Analysis (Validation set)",
   "id": "b73c4ab31f4cc3a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Residual = true - predict\n",
    "residuals = y_val_full - y_val_pred\n",
    "\n",
    "# 1. Residual distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, kde=True, bins=30, color='salmon')\n",
    "plt.title(\"Residual Distribution (Validation Set)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Residual vs Prediction\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_val_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title(\"Residuals vs. Predicted Values\")\n",
    "plt.xlabel(\"Predicted T2M\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3. QQ plotÔºàNormality testÔºâ\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(5, 5))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "42c5929e8460fc4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Importances",
   "id": "9f2b488ccd11be24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyse which features are important, showing top 15\n",
    "importances = pd.Series(final_model.feature_importances_, index=X_full.columns)\n",
    "top_features = importances.sort_values(ascending=True).tail(15)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index, color='skyblue')\n",
    "plt.title(\"Top 15 Important Features (Lag Dataset)\", fontsize=14)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6e179252b3b23aea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Improvement by nearby features\n",
    "This part we will compare the model performances between original dataset and strengthened dataset to understand the improvement brought by the nearby features."
   ],
   "id": "cde2a1dd5e3c9e37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standard dataset: training and prediction\n",
   "id": "8b0c12f8a6387f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load standard dataset\n",
    "df_std = prepare_dataset(option=\"standard\")\n",
    "\n",
    "# split into train and validation set\n",
    "X_std = df_std.drop(columns=[\"t2m\"])\n",
    "y_std = df_std[\"t2m\"]\n",
    "X_train_std, X_val_std, y_train_std, y_val_std = train_test_split(X_std, y_std, test_size=0.2, random_state=42)\n",
    "\n",
    "# model training\n",
    "rf_std = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_std.fit(X_train_std, y_train_std)\n",
    "\n",
    "# prediction\n",
    "y_pred_std = rf_std.predict(X_val_std)\n",
    "\n",
    "# save prediction result\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_val_std,\n",
    "    \"y_pred\": y_pred_std\n",
    "}).to_csv(\"y_val_pred_std.csv\", index=False)\n",
    "\n",
    "print(\"Standard model has been trained and saved as y_val_pred_std.csv\")\n"
   ],
   "id": "3454b948c8443fba",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis and visualisation",
   "id": "5b9e23b7edde6395"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load two sets\n",
    "std_df = pd.read_csv(\"y_val_pred_std.csv\")\n",
    "lag_df = pd.read_csv(\"rf_y_valid_predictions.csv\")\n",
    "\n",
    "# metrics computing\n",
    "metrics = {\n",
    "    \"RMSE\": lambda y, yhat: mean_squared_error(y, yhat, squared=False),\n",
    "    \"R¬≤\": r2_score,\n",
    "    \"MAE\": mean_absolute_error\n",
    "}\n",
    "\n",
    "# create comparing chart\n",
    "eval_df = pd.DataFrame({\n",
    "    metric: [\n",
    "        func(std_df[\"y_true\"], std_df[\"y_pred\"]),\n",
    "        func(lag_df[\"y_true\"], lag_df[\"y_pred\"])\n",
    "    ]\n",
    "    for metric, func in metrics.items()\n",
    "}, index=[\"Standard\", \"Lag+Nearby\"])\n",
    "\n",
    "# visualisations\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, metric in enumerate(eval_df.columns):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.barplot(x=eval_df.index, y=eval_df[metric], palette=\"Set2\")\n",
    "    plt.title(metric)\n",
    "    plt.grid(True)\n",
    "    plt.ylabel(metric)\n",
    "\n",
    "plt.suptitle(\"üìä Evaluation Comparison: Standard vs Lag+Nearby Features\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "id": "d29e565f6e115e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
