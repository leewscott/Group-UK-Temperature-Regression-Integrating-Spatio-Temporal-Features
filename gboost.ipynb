# %% [markdown]
# # ID5059 Group Project - Gradient Boosting Model for Temperature Prediction
# 
# This notebook implements a Gradient Boosting model to predict temperature (t2m) from weather data.

# %% [markdown]
# ## 1. Import Libraries

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# For preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV

# For modeling
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance

# For visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# %% [markdown]
# ## 2. Load Data

# %%
# Function to load data from file
def loadData(fileName, filePath="./", delimiter=","):
    path = Path(filePath) / fileName
    if not path.is_file():
        raise Exception(f"File not found: {path}")
    
    return pd.read_csv(path, sep=delimiter)

# Load training data
filePath = "./"
dataTrain = loadData("train.csv", filePath)

# Display shape and first few rows
print(f"Training data shape: {dataTrain.shape}")
dataTrain.head()

# %% [markdown]
# ## 3. Data Preprocessing

# %% [markdown]
# ### 3.1 Basic Preprocessing

# %%
# Drop ID column - not useful for prediction
dataTrain = dataTrain.drop(["id"], axis=1)

# Convert valid_time to datetime
dataTrain['valid_time'] = pd.to_datetime(dataTrain['valid_time'])

# Extract datetime components
def convertDateTimeToComponents(data, column):
    data["year"] = data[column].dt.year
    data["month"] = data[column].dt.month
    data["day"] = data[column].dt.day
    data["hour"] = data[column].dt.hour
    data["season"] = (data["month"] % 12 + 3) // 3  # Winter: 1, Spring: 2, Summer: 3, Fall: 4
    
    return data

dataTrain = convertDateTimeToComponents(dataTrain, "valid_time")

# Drop valid_time - we now have more useful time components
dataTrain = dataTrain.drop(["valid_time"], axis=1)

# Display the preprocessed data
print(dataTrain.shape)
dataTrain.head()

# %% [markdown]
# ### 3.2 Check for Missing Values

# %%
# Check for missing values
print("Missing values in training data:")
print(dataTrain.isnull().sum())

# %% [markdown]
# ### 3.3 One-hot Encoding for Categorical Variables

# %%
# Identify categorical columns that need one-hot encoding
categorical_cols = ['month', 'day', 'hour', 'season', 'year']

# Preview unique values
for col in categorical_cols:
    print(f"{col}: {dataTrain[col].nunique()} unique values")

# %% [markdown]
# ### 3.4 Feature Engineering and Preprocessing Pipeline

# %%
# Create a smaller sample for development (to speed up testing)
sample_size = 100000  # Adjust based on your computational resources
dataTrain_sample = dataTrain.sample(n=sample_size, random_state=RANDOM_SEED)

# Split features and target
X = dataTrain_sample.drop('t2m', axis=1)
y = dataTrain_sample['t2m']

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_SEED
)

print(f"Training set: {X_train.shape}, Validation set: {X_val.shape}")

# Identify numeric and categorical columns
numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()
numeric_cols = [col for col in numeric_cols if col not in categorical_cols]

print("Numeric columns:", numeric_cols)
print("Categorical columns:", categorical_cols)

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# %% [markdown]
# ## 4. Model Training

# %% [markdown]
# ### 4.1 Initial Gradient Boosting Model

# %%
# Define the model pipeline
gb_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', GradientBoostingRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_SEED
    ))
])

# Train the model
print("Training the initial Gradient Boosting model...")
gb_pipeline.fit(X_train, y_train)

# %% [markdown]
# ### 4.2 Model Evaluation

# %%
# Make predictions
y_train_pred = gb_pipeline.predict(X_train)
y_val_pred = gb_pipeline.predict(X_val)

# Calculate metrics
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
val_mae = mean_absolute_error(y_val, y_val_pred)

print(f"Training RMSE: {train_rmse:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}")
print(f"Training R²: {train_r2:.4f}")
print(f"Validation R²: {val_r2:.4f}")
print(f"Training MAE: {train_mae:.4f}")
print(f"Validation MAE: {val_mae:.4f}")

# %% [markdown]
# ### 4.3 Visualization of Results

# %%
# Visualize predictions vs actual values
plt.figure(figsize=(12, 6))
plt.scatter(y_val, y_val_pred, alpha=0.3)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')
plt.xlabel('Actual Temperature (K)')
plt.ylabel('Predicted Temperature (K)')
plt.title('Gradient Boosting: Actual vs Predicted Temperature')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Visualize residuals
residuals = y_val - y_val_pred
plt.figure(figsize=(12, 6))
plt.scatter(y_val_pred, residuals, alpha=0.3)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Temperature (K)')
plt.ylabel('Residuals (K)')
plt.title('Gradient Boosting: Residuals Plot')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Histogram of residuals
plt.figure(figsize=(12, 6))
plt.hist(residuals, bins=50, alpha=0.7)
plt.axvline(x=0, color='r', linestyle='--')
plt.xlabel('Residual Value (K)')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 4.4 Feature Importance

# %%
# Extract feature names after transformation
cat_feature_names = list(gb_pipeline.named_steps['preprocessor']
                        .named_transformers_['cat']
                        .get_feature_names_out(categorical_cols))
feature_names = numeric_cols + cat_feature_names

# Get feature importances
gb_model = gb_pipeline.named_steps['model']
feature_importances = gb_model.feature_importances_

# Map importances to feature names
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort by importance
importance_df = importance_df.sort_values('Importance', ascending=False).head(20)

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Top 20 Feature Importances in Gradient Boosting Model')
plt.tight_layout()
plt.show()

# %% [markdown]
# ## 5. Hyperparameter Tuning

# %%
# Define the parameter grid for grid search
param_grid = {
    'model__n_estimators': [50, 100, 200],
    'model__learning_rate': [0.05, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4]
}

# NOTE: Uncomment this section to perform hyperparameter tuning
# Warning: This can take a significant amount of time
"""
# Create a smaller subset for grid search if necessary
X_train_small = X_train.sample(frac=0.3, random_state=RANDOM_SEED)
y_train_small = y_train[X_train_small.index]

grid_search = GridSearchCV(
    gb_pipeline,
    param_grid,
    scoring='neg_root_mean_squared_error',
    cv=3,
    n_jobs=-1,
    verbose=1
)

print("Performing grid search...")
grid_search.fit(X_train_small, y_train_small)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best RMSE: {-grid_search.best_score_:.4f}")

# Use the best model found
best_model = grid_search.best_estimator_
"""

# %% [markdown]
# ## 6. Final Model Training and Evaluation

# %%
# Based on hyperparameter tuning (or manually setting better parameters)
final_gb_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', GradientBoostingRegressor(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=5,
        min_samples_split=5,
        min_samples_leaf=2,
        subsample=0.8,
        random_state=RANDOM_SEED
    ))
])

# Train the final model
print("Training the final Gradient Boosting model...")
final_gb_pipeline.fit(X_train, y_train)

# Make predictions with the final model
y_train_pred_final = final_gb_pipeline.predict(X_train)
y_val_pred_final = final_gb_pipeline.predict(X_val)

# Calculate metrics
train_rmse_final = np.sqrt(mean_squared_error(y_train, y_train_pred_final))
val_rmse_final = np.sqrt(mean_squared_error(y_val, y_val_pred_final))
train_r2_final = r2_score(y_train, y_train_pred_final)
val_r2_final = r2_score(y_val, y_val_pred_final)

print(f"Final Model - Training RMSE: {train_rmse_final:.4f}")
print(f"Final Model - Validation RMSE: {val_rmse_final:.4f}")
print(f"Final Model - Training R²: {train_r2_final:.4f}")
print(f"Final Model - Validation R²: {val_r2_final:.4f}")

# %% [markdown]
# ## 7. Nearby Features Analysis

# %%
# Create spatial lag features to incorporate nearby points
# Note: This is a simplified approach - for a real model you might want to do more sophisticated spatial analysis

# Function to add spatial lag features
def add_spatial_lags(df, feature_cols, lat_step=0.25, lon_step=0.25):
    # Make a copy of the dataframe to avoid modifying the original
    df_copy = df.copy()
    
    # Create a dictionary to store the resulting dataframe
    result_df = df_copy.copy()
    
    # Directions for spatial lags: north, south, east, west
    directions = [
        ('north', lat_step, 0),
        ('south', -lat_step, 0),
        ('east', 0, lon_step),
        ('west', 0, -lon_step)
    ]
    
    # Add spatial lag features
    for direction, lat_offset, lon_offset in directions:
        # Create adjusted lat/lon
        df_copy['adj_lat'] = df_copy['latitude'] + lat_offset
        df_copy['adj_lon'] = df_copy['longitude'] + lon_offset
        
        # Join with the original dataframe to get values from nearby points
        # NOTE: This is a simplified approach and may not work well with the full dataset
        # For a real implementation, you would use a more efficient spatial join
        
        # For demonstration, we'll just create random values as placeholders
        for col in feature_cols:
            lag_col_name = f"{col}_{direction}_lag"
            # This is just a placeholder - in reality you would do a proper spatial join
            result_df[lag_col_name] = df_copy[col] + np.random.normal(0, 0.5, size=len(df_copy))
    
    return result_df

# NOTE: Uncomment this section to add spatial lag features
"""
# Select features to create lags for
spatial_feature_cols = ['t2m', 'sp', 'tp']

# Create a small sample for demonstration
df_sample = dataTrain.sample(1000, random_state=RANDOM_SEED)

# Add spatial lag features
df_with_lags = add_spatial_lags(df_sample, spatial_feature_cols)

# Check the new columns
print("Columns with spatial lags:", [col for col in df_with_lags.columns if '_lag' in col])
df_with_lags[['latitude', 'longitude'] + [col for col in df_with_lags.columns if '_lag' in col]].head()
"""

# %% [markdown]
# ## 8. Predictions on Test Set

# %%
# Load test data
dataTest = loadData("test.csv", filePath)
print(f"Test data shape: {dataTest.shape}")
dataTest.head()

# %%
# Preprocess test data (same steps as training data)
dataTest = dataTest.drop(["id"], axis=1)
dataTest['valid_time'] = pd.to_datetime(dataTest['valid_time'])
dataTest = convertDateTimeToComponents(dataTest, "valid_time")

# Keep valid_time for creating predictions.csv
test_valid_time = dataTest['valid_time'].copy()
dataTest = dataTest.drop(["valid_time"], axis=1)

# Extract the specific test subset we need to submit
test_subset = dataTest[(dataTest['latitude'] == 56.25) & (dataTest['longitude'] == -2.75)]
print(f"Test subset shape: {test_subset.shape}")

# %%
# Train the final model on the full training set
# NOTE: For the actual submission, you would want to train on the entire dataset
print("Training the final model on the full dataset...")

# For demonstration, we'll use a sample of the full dataset
final_train_sample = dataTrain.sample(n=sample_size * 2, random_state=RANDOM_SEED)
X_full = final_train_sample.drop('t2m', axis=1)
y_full = final_train_sample['t2m']

final_gb_pipeline.fit(X_full, y_full)

# Make predictions on the test subset
test_predictions = final_gb_pipeline.predict(test_subset)

# Create predictions DataFrame
predictions_df = pd.DataFrame({
    'valid_time': test_subset.index.map(lambda idx: test_valid_time[idx]),
    'prediction': test_predictions
})

# Save predictions to CSV
predictions_df.to_csv('predictions.csv', index=False)
print("Predictions saved to predictions.csv")

# Display a sample of predictions
predictions_df.head()

# %% [markdown]
# ## 9. Summary and Next Steps
# 
# In this notebook, we've implemented a Gradient Boosting Regressor to predict temperature (t2m) from weather data. We've:
# 
# 1. Loaded and preprocessed the data
# 2. Created a preprocessing pipeline for feature scaling and one-hot encoding
# 3. Trained an initial model and evaluated its performance
# 4. Visualized predictions, residuals, and feature importances
# 5. (Optionally) Performed hyperparameter tuning
# 6. Trained a final model with improved parameters
# 7. Made predictions on the test set
# 
# **Next steps could include:**
# 
# - Further feature engineering (e.g., creating additional weather-related features)
# - Proper implementation of spatial lag features to capture nearby temperature patterns
# - Temporal feature engineering (e.g., lag features from previous time steps)
# - Ensemble with other models (Random Forest, Neural Networks, etc.)
# - Testing different boosting algorithms (XGBoost, LightGBM, CatBoost)
